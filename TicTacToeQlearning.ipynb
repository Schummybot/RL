{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550c6c8-d3c8-4103-8f49-7d103f3e2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6575e98-6e1e-4420-af99-49b4c9421574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [''] * 9  # Empty board\n",
    "        self.current_winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [''] * 9\n",
    "        self.current_winner = None\n",
    "        return tuple(self.board)\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i, x in enumerate(self.board) if x == '']\n",
    "\n",
    "    def make_move(self, position, player):\n",
    "        if self.board[position] == '':\n",
    "            self.board[position] = player\n",
    "            if self.winner(player):\n",
    "                self.current_winner = player\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def winner(self, player):\n",
    "        # Check rows, columns, diagonals\n",
    "        win_conditions = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # columns\n",
    "            [0, 4, 8], [2, 4, 6]  # diagonals\n",
    "        ]\n",
    "        for condition in win_conditions:\n",
    "            if all(self.board[i] == player for i in condition):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_draw(self):\n",
    "        return all(self.board) and self.current_winner is None\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, name = 'X', alpha=0.1, gamma=0.5, epsilon=0.8):\n",
    "        self.name = name\n",
    "        self.q_table = defaultdict(float)  # Q-values\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def choose_action(self, state, actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = self.exploit(state, actions)\n",
    "        return action\n",
    "\n",
    "    def exploit(self, state, actions):\n",
    "        q_values = [self.q_table[(state, a)] for a in actions]\n",
    "        max_q = max(q_values)\n",
    "        action = actions[q_values.index(max_q)] \n",
    "        return action\n",
    "\n",
    "    def update_q(self, state, actions):\n",
    "        max_future_q = max([self.q_table[(state, action)] for action in actions], default=0) \n",
    "        current_q = self.q_table[(self.state, self.action)]\n",
    "        self.q_table[(self.state, self.action)] = current_q + self.alpha * (self.reward + self.gamma * max_future_q - current_q)\n",
    "\n",
    "def train(agent_x, agent_o, env, episodes=10000, decay_rate=0.995):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        current_player = agent_x\n",
    "        agent_x.action = None\n",
    "        agent_o.action = None\n",
    "\n",
    "        while not done:\n",
    "            actions = env.available_actions()\n",
    "            action = current_player.choose_action(state, actions)\n",
    "\n",
    "            env.make_move(action, current_player.name)\n",
    "            next_state = tuple(env.board)\n",
    "            current_player.reward = 0\n",
    "            done = env.current_winner is not None or env.is_draw()\n",
    "\n",
    "            if current_player.action is not None:\n",
    "                current_player.update_q(state, actions)\n",
    "            \n",
    "            current_player.state = state\n",
    "            current_player.action = action\n",
    "                      \n",
    "            state = next_state\n",
    "                \n",
    "            current_player = agent_o if current_player.name == 'X' else agent_x\n",
    "        actions = []\n",
    "        if env.current_winner is not None:\n",
    "            current_player.reward = -1\n",
    "            current_player.update_q(state, actions)\n",
    "            winner =  agent_o if current_player.name == 'X' else agent_x\n",
    "            winner.reward = 1\n",
    "            winner.update_q(state, actions)\n",
    "        else:\n",
    "            current_player.update_q(state, actions)\n",
    "        if episode%2000 == 0:\n",
    "            agent_x.epsilon *= decay_rate\n",
    "            agent_o.epsilon *= decay_rate\n",
    "            print(len(agent_x.q_table), 'epsilon:', agent_x.epsilon)\n",
    "            for i in range(9):\n",
    "                print(i, agent_x.q_table[(tuple(['']*9), i)])\n",
    "\n",
    "# Initialize environment and agents\n",
    "env = TicTacToe()\n",
    "agent_x = QLearningAgent()\n",
    "agent_o = QLearningAgent(name='O')\n",
    "\n",
    "train(agent_x, agent_o, env, episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03abaf3-63fc-40db-be0e-247d620991ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with(agent_x):\n",
    "    env=TicTacToe()\n",
    "\n",
    "    agent_x.state = tuple(env.board)\n",
    "    done = False\n",
    "    current_player = 'X'  \n",
    "\n",
    "    while not done:\n",
    "        if current_player != agent_x.name:  # Human player\n",
    "            print(\"Your turn. Choose a position (0-8):\")\n",
    "            try:\n",
    "                position = int(input().strip())\n",
    "                if position not in env.available_actions():\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number between 0 and 8.\")\n",
    "                continue\n",
    "\n",
    "        else:  # AI's turn\n",
    "            print(\"Agent's turn...\")\n",
    "            position = agent_x.exploit(tuple(env.board), env.available_actions())\n",
    "\n",
    "        # Make the move and update the board\n",
    "        env.make_move(position, current_player)\n",
    "        print_board(env.board)\n",
    "\n",
    "        # Check for game end conditions\n",
    "        if env.current_winner:\n",
    "            if current_player != agent_x.name:\n",
    "                print(\"Congratulations! You won!\")\n",
    "            else:\n",
    "                print(\"Agent wins. Better luck next time!\")\n",
    "            done = True\n",
    "        elif env.is_draw():\n",
    "            print(\"It's a draw!\")\n",
    "            done = True\n",
    "\n",
    "        # Switch players\n",
    "        current_player = 'X' if current_player == 'O' else 'O'\n",
    "\n",
    "    print(\"Game over!\")\n",
    "\n",
    "\n",
    "def print_board(board):\n",
    "    # Helper function to print the board\n",
    "    for i in range(0, 9, 3):\n",
    "        print('|'.join([board[i + j] if board[i + j] != '' else str(i + j) for j in range(3)]))\n",
    "    print(\"-\" * 5)\n",
    "    \n",
    "play_with(agent_o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xu",
   "language": "python",
   "name": "xu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
